{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import twitter\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import lxml.html as html\n",
    "from urllib2 import urlopen\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import time\n",
    "import sklearn.cross_validation as cv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TJLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._news_pages = [\"https://tjournal.ru/paper/page/{}\"]\n",
    "        self._month_map = {u\"января\":\"01\", u\"февраля\":\"02\", u\"марта\":\"03\", u\"апреля\":\"04\", u\"мая\":\"05\", u\"июня\":\"06\", u\"июля\":\"07\", u\"августа\":\"08\", u\"сентября\":\"09\", u\"октября\":\"10\", u\"ноября\":\"11\", u\"декабря\":\"12\"}\n",
    "\n",
    "    def get_news_uri(self, min_index=10, count=30):\n",
    "        \"\"\"\n",
    "\n",
    "        :param min_index: int, индекс страницы, с которой нужно начать поиск\n",
    "        :param count: int, количество страниц, которые нужно скачать\n",
    "        :return: list. список ссылок на новости\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for news_page in self._news_pages:\n",
    "            \n",
    "            for i in range(count):\n",
    "                page = html.parse(urlopen(news_page.format(i+min_index)))\n",
    "                divs = page.getroot().find_class('b-articles__b__title')\n",
    "\n",
    "                for div in divs:\n",
    "                    links.append(div.getchildren()[1].get(\"href\"))\n",
    "                    \n",
    "        return links\n",
    "\n",
    "    \n",
    "    def _parse_date(self, date):\n",
    "        date = date.replace(\",\", \"\")\n",
    "        date = date.split(\" \")\n",
    "        \n",
    "        converted_date = date[2]\n",
    "        converted_date +=\"-\"+self._month_map[date[1]]\n",
    "        converted_date +=\"-\"+date[0]\n",
    "        \n",
    "        converted_date +=\" \"+date[3]\n",
    "        \n",
    "        return converted_date\n",
    "        \n",
    "\n",
    "    def get_link_info(self, link):\n",
    "        \"\"\"\n",
    "\n",
    "        :param link: str, url страницы с tjournal, для которой нужно собрать информацию\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        page = html.parse(urlopen(link))\n",
    "        root = page.getroot()\n",
    "\n",
    "        # заголовок\n",
    "        title = root.find_class(\"b-article__title\")\n",
    "        title = title[0].find(\"h1\").text\n",
    "\n",
    "        # парсим количество просмотров\n",
    "        view = root.get_element_by_id(\"hitsCount\").text\n",
    "        view = view.replace(\" \", \"\")\n",
    "        view = int(view)\n",
    "\n",
    "        # Количество комментариев\n",
    "        comments = root.find_class(\"b-article__infoline__comments\")\n",
    "        comment = int(comments[0].find(\"b\").text.replace(\" \", \"\"))\n",
    "\n",
    "        \n",
    "        # Дата\n",
    "        date = root.find_class(\"b-article__infoline__date\")\n",
    "        date = self._parse_date(date[0].text)\n",
    "        \n",
    "        \n",
    "        # Type\n",
    "        news_type = \"TJ_P\"\n",
    "            \n",
    "        return {\n",
    "            \"url\": link, \n",
    "            \"title\": title, \n",
    "            \"views\": view, \n",
    "            \"comments\": comment, \n",
    "            \"created_date\": date, \n",
    "            \"collect_date\": datetime.today().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "            \"type\": news_type\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "    def get_tj_news_info(self, min_index=1, count=1, first_date=\"2017-01-01\", last_date=\"2010-01-01\"):\n",
    "        \"\"\"\n",
    "        :param min_index: int, индекс минимальной страницы, откуда начинаем поиск\n",
    "        :param count: int, количество страниц, по которым ищем\n",
    "        :first_date: время первой новости, которую мы скачаем\n",
    "        :last_date: время последней новости, которую мы скачаем\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        links = self.get_news_uri(min_index=min_index, count=count)\n",
    "        link_info_list = []\n",
    "        i = 0\n",
    "        is_break = False\n",
    "        for link in links:\n",
    "            link_info = self.get_link_info(link)\n",
    "            # Если заданное время не подходит\n",
    "            if link_info[\"created_date\"] > first_date or link_info[\"created_date\"] < last_date:\n",
    "                continue\n",
    "    \n",
    "            link_info_list.append(link_info)\n",
    "            \n",
    "            i+=1\n",
    "            if i%10 == 0:\n",
    "                print \"Скачали \", i, \" страниц\"\n",
    "\n",
    "        return link_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VCLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._news_pages = \"https://api.vc.ru/1/paper\"\n",
    "        self._month_map = {u\"января\":\"01\", u\"февраля\":\"02\", u\"марта\":\"03\", u\"апреля\":\"04\", u\"мая\":\"05\", u\"июня\":\"06\", u\"июля\":\"07\", u\"августа\":\"08\", u\"сентября\":\"09\", u\"октября\":\"10\", u\"ноября\":\"11\", u\"декабря\":\"12\"}\n",
    "\n",
    "    def get_news_uri(self, min_index=10, count=30):\n",
    "        \"\"\"\n",
    "\n",
    "        :param min_index: int, индекс страницы, с которой нужно начать поиск\n",
    "        :param count: int, количество страниц, которые нужно скачать\n",
    "        :return: list. список ссылок на новости\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        text = requests.get(self._news_pages).text\n",
    "        json_req = json.loads(text)\n",
    "    \n",
    "        for news in json_req:\n",
    "            links.append(news[\"url\"])\n",
    "\n",
    "        return links\n",
    "\n",
    "    \n",
    "    def _parse_date(self, date):\n",
    "        date = date.replace(\",\", \"\")\n",
    "        date = date.split(\" \")\n",
    "        \n",
    "        converted_date = date[2]\n",
    "        converted_date +=\"-\"+self._month_map[date[1]]\n",
    "        converted_date +=\"-\"+date[0]\n",
    "        \n",
    "        converted_date +=\" \"+date[3]\n",
    "        \n",
    "        return converted_date\n",
    "        \n",
    "\n",
    "    def get_link_info(self, link):\n",
    "        \"\"\"\n",
    "\n",
    "        :param link: str, url страницы с vc, для которой нужно собрать информацию\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        page = html.parse(urlopen(link))\n",
    "        root = page.getroot()\n",
    "\n",
    "        # заголовок\n",
    "        title = root.find_class(\"b-article__head\")\n",
    "        title = title[0].find(\"h1\").text\n",
    "\n",
    "        # парсим количество просмотров\n",
    "        view = root.get_element_by_id(\"hitsCount\").text\n",
    "        view = view.replace(\" \", \"\")\n",
    "        view = int(view)\n",
    "\n",
    "        # Количество комментариев\n",
    "        comments = root.find_class(\"ccount\")[0].text\n",
    "        comment = int(comments.replace(\" \", \"\"))\n",
    "\n",
    "        # Теги\n",
    "        tags = root.find_class(\"b-tags__tag\")\n",
    "        tag_list = []\n",
    "        for tag in tags:\n",
    "            tag_list.append(tag.text)\n",
    "        \n",
    "        # Дата\n",
    "        date = root.find_class(\"b-article__infopanel__date\")\n",
    "        date = self._parse_date(date[0].text)\n",
    "        \n",
    "\n",
    "        return {\n",
    "                \"url\": link, \n",
    "                \"title\": title, \n",
    "                \"views\": view, \n",
    "                \"comments\": comment, \n",
    "                \"created_date\": date, \n",
    "                \"collect_date\": datetime.today().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                \"type\": \"VC\"\n",
    "               }\n",
    "\n",
    "\n",
    "    def get_tj_news_info(self, min_index=1, count=30, first_date=\"2010-01-01\", last_date=\"2017-01-01\"):\n",
    "        \"\"\"\n",
    "        :param min_index: int, индекс минимальной страницы, откуда начинаем поиск\n",
    "        :param count: int, количество страниц, по которым ищем\n",
    "        :first_date: str, дата и время первой (самой новой) новости\n",
    "        :last_date: str, дата и время последней(самой старой) новости\n",
    "        \n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        links = self.get_news_uri(min_index=min_index, count=count)\n",
    "        link_info_list = []\n",
    "        i = 0\n",
    "        \n",
    "        for link in links:\n",
    "            \n",
    "            link_info = self.get_link_info(link)\n",
    "            # Если заданное время не подходит\n",
    "            if link_info[\"created_date\"] < first_date or link_info[\"created_date\"] > last_date:\n",
    "                print \"continue\"\n",
    "                continue\n",
    "            \n",
    "            link_info_list.append(link_info)\n",
    "            \n",
    "            i+=1\n",
    "            if i%10 == 0:\n",
    "                print \"Скачали \", i, \" страниц\"\n",
    "\n",
    "        return link_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLEEP_TIME = 60*60\n",
    "COMMENTS_COUNT_FILE = \"comments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-12-06 14:14'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "while (i<24):\n",
    "    i+=1\n",
    "    tj_pages = TJLoader().get_tj_news_info()\n",
    "    vc_pages = VCLoader().get_tj_news_info()\n",
    "    new_df = pd.DataFrame(tj_pages+vc_pages)\n",
    "    \n",
    "    prev_df = pd.read_csv(OUT_NEWS_FILE, sep=\",\")\n",
    "    df = (prev_df.append(new_df)).reset_index(drop=True)\n",
    "    \n",
    "    print \"Сейчас скачано {} записей. Номер итерации {}\".format(len(df), i)\n",
    "    \n",
    "    df.to_csv(COMMENTS_COUNT_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)    \n",
    "    \n",
    "    time.sleep(SLEEP_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сейчас скачано 2 записей. Номер итерации 1\n"
     ]
    }
   ],
   "source": [
    "print \"Сейчас скачано {} записей. Номер итерации {}\".format(2, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
