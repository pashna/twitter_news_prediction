{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import twitter\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import lxml.html as html\n",
    "from urllib2 import urlopen\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import time\n",
    "import sklearn.cross_validation as cv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUT_NEWS_FILE = \"news.csv\"\n",
    "OUT_TWITTER_FILE = \"twitter.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TJLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._news_pages = [\"https://tjournal.ru/paper/page/{}\"]#, \"https://tjournal.ru/club/news/recent/page/{}\"]\n",
    "        self._month_map = {u\"января\":\"01\", u\"февраля\":\"02\", u\"марта\":\"03\", u\"апреля\":\"04\", u\"мая\":\"05\", u\"июня\":\"06\", u\"июля\":\"07\", u\"августа\":\"08\", u\"сентября\":\"09\", u\"октября\":\"10\", u\"ноября\":\"11\", u\"декабря\":\"12\"}\n",
    "\n",
    "    def get_news_uri(self, min_index=10, count=30):\n",
    "        \"\"\"\n",
    "\n",
    "        :param min_index: int, индекс страницы, с которой нужно начать поиск\n",
    "        :param count: int, количество страниц, которые нужно скачать\n",
    "        :return: list. список ссылок на новости\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for news_page in self._news_pages:\n",
    "            \n",
    "            for i in range(count):\n",
    "                page = html.parse(urlopen(news_page.format(i+min_index)))\n",
    "                divs = page.getroot().find_class('b-articles__b__title')\n",
    "\n",
    "                for div in divs:\n",
    "                    links.append(div.getchildren()[1].get(\"href\"))\n",
    "                    \n",
    "        return links\n",
    "\n",
    "    \n",
    "    def _parse_date(self, date):\n",
    "        date = date.replace(\",\", \"\")\n",
    "        date = date.split(\" \")\n",
    "        \n",
    "        converted_date = date[2]\n",
    "        converted_date +=\"-\"+self._month_map[date[1]]\n",
    "        converted_date +=\"-\"+date[0]\n",
    "        \n",
    "        converted_date +=\" \"+date[3]\n",
    "        \n",
    "        return converted_date\n",
    "        \n",
    "\n",
    "    def get_link_info(self, link):\n",
    "        \"\"\"\n",
    "\n",
    "        :param link: str, url страницы с tjournal, для которой нужно собрать информацию\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        page = html.parse(urlopen(link))\n",
    "        root = page.getroot()\n",
    "\n",
    "        # заголовок\n",
    "        title = root.find_class(\"b-article__title\")\n",
    "        title = title[0].find(\"h1\").text\n",
    "\n",
    "        # парсим количество просмотров\n",
    "        view = root.get_element_by_id(\"hitsCount\").text\n",
    "        view = view.replace(\" \", \"\")\n",
    "        view = int(view)\n",
    "\n",
    "        # Количество комментариев\n",
    "        comments = root.find_class(\"b-article__infoline__comments\")\n",
    "        comment = int(comments[0].find(\"b\").text.replace(\" \", \"\"))\n",
    "\n",
    "        # Теги\n",
    "        tags = root.find_class(\"b-article__tags__tag\")\n",
    "        tag_list = []\n",
    "        for tag in tags:\n",
    "            tag_list.append(tag.text)\n",
    "        \n",
    "        # Дата\n",
    "        date = root.find_class(\"b-article__infoline__date\")\n",
    "        date = self._parse_date(date[0].text)\n",
    "        \n",
    "        # Первоисточник\n",
    "        source = root.find_class(\"b-article__link\")\n",
    "        if len(source) != 0:\n",
    "            source = source[0].getchildren()[0].getchildren()[1].getchildren()[0].text\n",
    "        else:\n",
    "            source = None\n",
    "        \n",
    "        # Type\n",
    "        if \"tjournal.ru/c/\" in link:\n",
    "            news_type = \"TJ_C\"\n",
    "        else:\n",
    "            news_type = \"TJ_P\"\n",
    "            \n",
    "        return {\n",
    "            \"url\": link, \n",
    "            \"title\": title, \n",
    "            \"views\": view, \n",
    "            \"comments\": comment, \n",
    "            \"tags\": tag_list, \n",
    "            \"date\": date, \n",
    "            \"type\": news_type, \n",
    "            \"source\": source}\n",
    "\n",
    "\n",
    "    def get_tj_news_info(self, min_index=1, count=30, first_date=\"2017-01-01\", last_date=\"2010-01-01\"):\n",
    "        \"\"\"\n",
    "        :param min_index: int, индекс минимальной страницы, откуда начинаем поиск\n",
    "        :param count: int, количество страниц, по которым ищем\n",
    "        :first_date: время первой новости, которую мы скачаем\n",
    "        :last_date: время последней новости, которую мы скачаем\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        links = self.get_news_uri(min_index=min_index, count=count)\n",
    "        link_info_list = []\n",
    "        i = 0\n",
    "        is_break = False\n",
    "        for link in links:\n",
    "            link_info = self.get_link_info(link)\n",
    "            # Если заданное время не подходит\n",
    "            if link_info[\"date\"] > first_date or link_info[\"date\"] < last_date:\n",
    "                continue\n",
    "    \n",
    "            link_info_list.append(link_info)\n",
    "            \n",
    "            i+=1\n",
    "            if i%10 == 0:\n",
    "                print \"Скачали \", i, \" страниц\"\n",
    "\n",
    "        return link_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VCLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._news_pages = \"https://api.vc.ru/1/paper\"\n",
    "        self._month_map = {u\"января\":\"01\", u\"февраля\":\"02\", u\"марта\":\"03\", u\"апреля\":\"04\", u\"мая\":\"05\", u\"июня\":\"06\", u\"июля\":\"07\", u\"августа\":\"08\", u\"сентября\":\"09\", u\"октября\":\"10\", u\"ноября\":\"11\", u\"декабря\":\"12\"}\n",
    "\n",
    "    def get_news_uri(self, min_index=10, count=30):\n",
    "        \"\"\"\n",
    "\n",
    "        :param min_index: int, индекс страницы, с которой нужно начать поиск\n",
    "        :param count: int, количество страниц, которые нужно скачать\n",
    "        :return: list. список ссылок на новости\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        text = requests.get(self._news_pages).text\n",
    "        json_req = json.loads(text)\n",
    "    \n",
    "        for news in json_req:\n",
    "            links.append(news[\"url\"])\n",
    "\n",
    "        return links\n",
    "\n",
    "    \n",
    "    def _parse_date(self, date):\n",
    "        date = date.replace(\",\", \"\")\n",
    "        date = date.split(\" \")\n",
    "        \n",
    "        converted_date = date[2]\n",
    "        converted_date +=\"-\"+self._month_map[date[1]]\n",
    "        converted_date +=\"-\"+date[0]\n",
    "        \n",
    "        converted_date +=\" \"+date[3]\n",
    "        \n",
    "        return converted_date\n",
    "        \n",
    "\n",
    "    def get_link_info(self, link):\n",
    "        \"\"\"\n",
    "\n",
    "        :param link: str, url страницы с vc, для которой нужно собрать информацию\n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        page = html.parse(urlopen(link))\n",
    "        root = page.getroot()\n",
    "\n",
    "        # заголовок\n",
    "        title = root.find_class(\"b-article__head\")\n",
    "        title = title[0].find(\"h1\").text\n",
    "\n",
    "        # парсим количество просмотров\n",
    "        view = root.get_element_by_id(\"hitsCount\").text\n",
    "        view = view.replace(\" \", \"\")\n",
    "        view = int(view)\n",
    "\n",
    "        # Количество комментариев\n",
    "        comments = root.find_class(\"ccount\")[0].text\n",
    "        comment = int(comments.replace(\" \", \"\"))\n",
    "\n",
    "        # Теги\n",
    "        tags = root.find_class(\"b-tags__tag\")\n",
    "        tag_list = []\n",
    "        for tag in tags:\n",
    "            tag_list.append(tag.text)\n",
    "        \n",
    "        # Дата\n",
    "        date = root.find_class(\"b-article__infopanel__date\")\n",
    "        date = self._parse_date(date[0].text)\n",
    "        \n",
    "\n",
    "        return {\"title\": title, \"views\": view, \"comments\": comment, \"tags\": tag_list, \"date\": date, \"url\": link, \"type\": \"VC\"}\n",
    "\n",
    "\n",
    "    def get_tj_news_info(self, min_index=1, count=30, first_date=\"2010-01-01\", last_date=\"2017-01-01\"):\n",
    "        \"\"\"\n",
    "        :param min_index: int, индекс минимальной страницы, откуда начинаем поиск\n",
    "        :param count: int, количество страниц, по которым ищем\n",
    "        :first_date: str, дата и время первой (самой новой) новости\n",
    "        :last_date: str, дата и время последней(самой старой) новости\n",
    "        \n",
    "        :return: dict с данными со страницы\n",
    "        \"\"\"\n",
    "        links = self.get_news_uri(min_index=min_index, count=count)\n",
    "        link_info_list = []\n",
    "        i = 0\n",
    "        \n",
    "        for link in links:\n",
    "            \n",
    "            link_info = self.get_link_info(link)\n",
    "            # Если заданное время не подходит\n",
    "            if link_info[\"date\"] > first_date or link_info[\"date\"] < last_date:\n",
    "                continue\n",
    "            \n",
    "            link_info_list.append(link_info)\n",
    "            \n",
    "            i+=1\n",
    "            if i%10 == 0:\n",
    "                print \"Скачали \", i, \" страниц\"\n",
    "\n",
    "        return link_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RSSLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #self._pages = [\"https://roem.ru/rss/roem-all-news.xml\", \"http://lifenews.ru/xml/feed.xml\", \"http://www.forbes.ru/newrss.xml\", \"http://rg.ru/xml/index.xml\", \"http://www.vesti.ru/vesti.rss\", \"http://lenta.ru/rss\", \"http://ria.ru/export/rss2/index.xml\"]\n",
    "        self._pages = [\"https://roem.ru/rss/roem-all-news.xml\", \"http://lifenews.ru/xml/feed.xml\", \"http://www.forbes.ru/newrss.xml\", \"http://www.vesti.ru/vesti.rss\", \"http://lenta.ru/rss\", \"http://ria.ru/export/rss2/index.xml\"]\n",
    "        self._month_dict = {\"Jan\":\"1\", \"Feb\":\"2\", \"Mar\":\"3\", \"Apr\":\"4\", \"May\":\"5\", \"Jun\":\"6\", \"Jul\":\"7\", \"Aug\":\"8\", \"Sep\":\"9\", \"Oct\":\"10\", \"Nov\":\"11\", \"Dec\":\"12\"}\n",
    "        \n",
    "        \n",
    "        self._UTC_TIME_ZONE = tz.gettz('Europe/London')\n",
    "        self._MOSCOW_TIME_ZONE = tz.gettz('Europe/Moscow')\n",
    "        self._RATE_LIMIT = \"[{u'message': u'Rate limit exceeded', u'code': 88}]\"\n",
    "        \n",
    "        \n",
    "    def _parse_date(self, date):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param date: str, дата в формате - \"Fri, 04 Dec 2015 14:45:39 +0000\"\n",
    "        :return: str, дата в человеческом, но буржуйском формате, да еще и в Московском часовом поясе\n",
    "        \"\"\"\n",
    "        timezone = date.split(\"+\")[1]\n",
    "        date = date.replace(\",\", \"\")\n",
    "        date_array = date.split(' ')\n",
    "        \n",
    "        time = date_array[4]\n",
    "        day = int(date_array[1])\n",
    "        month = self._month_dict[date_array[2]]\n",
    "        year = date_array[3]\n",
    "\n",
    "        date = str(year)+\"-\"+str(month)+\"-\"+str(day)+\" \"+time\n",
    "        date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        if (timezone != \"0300\"):\n",
    "            utc_date = date.replace(tzinfo=self._UTC_TIME_ZONE)\n",
    "            date = utc_date.astimezone(self._MOSCOW_TIME_ZONE)\n",
    "\n",
    "        # Обрезаем зону\n",
    "        date = str(date).split(\"+\")[0]\n",
    "        \n",
    "        # Обрезаем секунды\n",
    "        splited = date.split(\":\")\n",
    "        date = splited[0]+\":\"+splited[1]\n",
    "        return date\n",
    "    \n",
    "    \n",
    "    def _get_type(self, url):\n",
    "        url = url.replace(\"www\", \"\")\n",
    "        url = url.split(\"://\")[1]\n",
    "        url = url.split(\"/\")[0]\n",
    "        return url\n",
    "        \n",
    "    \n",
    "    def _get_hours_until_now(self, date):\n",
    "        news_date = datetime.strptime(date, '%Y-%m-%d %H:%M')\n",
    "        now = datetime.today()\n",
    "        return float((now-news_date).total_seconds()/3600)\n",
    "        \n",
    "    def get_news_array(self, hour=8):\n",
    "        \"\"\"\n",
    "        hour: int, время последней новости\n",
    "        \"\"\"\n",
    "        news_array = []\n",
    "        for url in self._pages:\n",
    "            news_type = self._get_type(url)\n",
    "            print url\n",
    "            \n",
    "            tree = ET.ElementTree(file=urlopen(url))\n",
    "            root = tree.getroot()\n",
    "            for i in root.iter('item'):\n",
    "                link = i.find('link').text\n",
    "                title = i.find('title').text\n",
    "                date_ = i.find('pubDate').text\n",
    "                date = self._parse_date(date_)\n",
    "                \n",
    "                news_info =  {\n",
    "                    \"title\": title,\n",
    "                    \"date\": date, \n",
    "                    \"url\": link, \n",
    "                    \"type\": news_type\n",
    "                }\n",
    "                \n",
    "                if self._get_hours_until_now(date) >= hour:\n",
    "                    news_array.append(news_info)\n",
    "                \n",
    "            \n",
    "        print \"Собрано \", len(news_array), \" новостей с RSS\"\n",
    "        return news_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TwitterLoader:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        CONSUMER_KEY = 'BOuuaMDhNhm6yx0rzqK8bMsbI'\n",
    "        CONSUMER_SECRET = '3DybJwlkXd2vU6R385yLA8yJblYJltLtwojySD9AVs04ShauZ0'\n",
    "\n",
    "        ACCESS_TOKEN_KEY = '3712177576-of3jzZ8gNmlPDfPjPyR0Ljw1Ao2IXdTqX9dZGDZ'\n",
    "        ACCESS_TOKEN_SECRET = 'Ky7iKwByHNXX3UMfuMhv6UgVx2IhjLo3KmwpsBQz35wtG'\n",
    "\n",
    "        self.api = twitter.Api(consumer_key=CONSUMER_KEY,\n",
    "                  consumer_secret=CONSUMER_SECRET,\n",
    "                  access_token_key=ACCESS_TOKEN_KEY,\n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)\n",
    "        self._month_dict = {\"Jan\":\"1\", \"Feb\":\"2\", \"Mar\":\"3\", \"Apr\":\"4\", \"May\":\"5\", \"Jun\":\"6\", \"Jul\":\"7\", \"Aug\":\"8\", \"Sep\":\"9\", \"Oct\":\"10\", \"Nov\":\"11\", \"Dec\":\"12\"}\n",
    "\n",
    "        self._UTC_TIME_ZONE = tz.gettz('Europe/London')\n",
    "        self._MOSCOW_TIME_ZONE = tz.gettz('Europe/Moscow')\n",
    "        self._RATE_LIMIT = \"[{u'message': u'Rate limit exceeded', u'code': 88}]\"\n",
    "\n",
    "\n",
    "\n",
    "    def _parse_date(self, date):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param date: str, дата в формате твиттера - \"Sat Nov 21 17:00:29 +0000 2015\"\n",
    "        :return: str, дата в человеческом, но буржуйском формате, да еще и в Московском часовом поясе\n",
    "        \"\"\"\n",
    "        date_array = date.split(' ')\n",
    "        month = self._month_dict[date_array[1]]\n",
    "        day = int(date_array[2])\n",
    "        time = date_array[3]\n",
    "        year = date_array[5]\n",
    "\n",
    "        date = str(year)+\"-\"+str(month)+\"-\"+str(day)+\" \"+time\n",
    "        date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        utc_date = date.replace(tzinfo=self._UTC_TIME_ZONE)\n",
    "        moscow_date = utc_date.astimezone(self._MOSCOW_TIME_ZONE)\n",
    "\n",
    "        return str(moscow_date).split(\"+\")[0]\n",
    "\n",
    "\n",
    "\n",
    "    def loadTweetWithLink(self, link, date):\n",
    "        \"\"\"\n",
    "\n",
    "        :param link: str, ключевое слово для поиска, вданном случае - ссылка на письмо\n",
    "        :param date: str, дата, до которой искать твиты\n",
    "        :return: список словариков с информацией о УНИКАЛЬНЫХ твитах по запросу link\n",
    "        \"\"\"\n",
    "        set_id = set()\n",
    "        tweet_list = []\n",
    "        result = self.api.GetSearch(term=link, until=date, count=100000)\n",
    "\n",
    "        for res in result:\n",
    "\n",
    "            tw_id = res.GetId()\n",
    "            # Если мы уже обрабатывали этот твит - идем дальше\n",
    "            if tw_id in set_id:\n",
    "                continue\n",
    "\n",
    "            retweeted_status = res.GetRetweeted_status()\n",
    "            # Если это не ретвит\n",
    "            if retweeted_status is None:\n",
    "                is_retweet = 0\n",
    "                retweeted_count = res.GetRetweetCount()\n",
    "                favorite_count = res.GetFavoriteCount()\n",
    "            else:\n",
    "                is_retweet = 1\n",
    "                retweeted_count = 0\n",
    "                favorite_count = 0\n",
    "            set_id.add(tw_id)\n",
    "\n",
    "            created_at = res.GetCreatedAt()\n",
    "            created_at = self._parse_date(created_at)\n",
    "\n",
    "            # Данные о пользователе\n",
    "            user = res.GetUser()\n",
    "            followers_count = user.followers_count\n",
    "            listed_count = user.listed_count\n",
    "            friends_count = user.friends_count\n",
    "            favourites_count = user.favourites_count\n",
    "            statuses_count = user.statuses_count\n",
    "\n",
    "\n",
    "            tw_dict= {\n",
    "                    \"url\": link,\n",
    "                    \"tw_id\":tw_id,\n",
    "                    \"retweeted_count\": retweeted_count,\n",
    "                    \"favorite_count\":favorite_count,\n",
    "                    \"is_retweet\": is_retweet,\n",
    "                    \"created_at\":created_at,\n",
    "                    \"user_followers_count\":followers_count,\n",
    "                    \"user_listed_count\": listed_count,\n",
    "                    \"user_friends_count\":friends_count,\n",
    "                    \"user_favourites_count\":favourites_count,\n",
    "                    \"user_statuses_count\":statuses_count\n",
    "                    }\n",
    "\n",
    "            tweet_list.append(tw_dict)\n",
    "\n",
    "        return tweet_list\n",
    "\n",
    "    def _get_next_date(self, date, days):\n",
    "        \"\"\"\n",
    "\n",
    "        :param date: str, дата\n",
    "        :param days: int, количество дней\n",
    "        :return: возвращает дату через days-дней после date\n",
    "        \"\"\"\n",
    "        date = datetime.strptime(date, '%Y-%m-%d %H:%M')\n",
    "        date += timedelta(days=days)\n",
    "        return str(date).split(' ')[0]\n",
    "\n",
    "\n",
    "    def load_tweets_by_term(self, news_list, days_after_news=2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param news_list: list(str), список словариков с информацией о новости\n",
    "        :param days_after_news:  int, количество дней после публикации новости, до которой искать\n",
    "        \"\"\"\n",
    "        result_list = []\n",
    "        i = 0\n",
    "        for news in news_list:\n",
    "            tweets = []\n",
    "            try:\n",
    "                until_date = self._get_next_date(news[\"date\"], days_after_news)\n",
    "                tweets = self.loadTweetWithLink(news[\"url\"], until_date)\n",
    "            except twitter.error.TwitterError as ex:\n",
    "                print str(ex)\n",
    "                if str(ex) == self._RATE_LIMIT:\n",
    "                    sleep_time = self.api.GetSleepTime(\"search/tweets\") #??? Почему-то не работает\n",
    "                    print \"Спим {} сек.\".format(sleep_time)\n",
    "                    time.sleep(sleep_time+2)\n",
    "                    tweets = self.loadTweetWithLink(news[\"url\"], until_date)\n",
    "\n",
    "            result_list+= tweets\n",
    "            \n",
    "            i+=1\n",
    "            if i%10 == 0:\n",
    "                print \"Собрано информация о\", i, \" новостях\"\n",
    "            \n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Подготавливаем  данные для анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_info = pd.read_csv(OUT_NEWS_FILE, sep=\",\")\n",
    "tweeter_data = pd.read_csv(OUT_TWITTER_FILE, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !!!!   !!!!!!!!!! !!!!!!!!!!1 !!!!!!!!!!!!\n",
    "# Сначала почищу данные. Там, где не материалы редакции - мусор, на который твитов то почти и нет!\n",
    "news_info = news_info[ (news_info['type'] == \"VC\") | (news_info[\"type\"] == \"TJ_P\")]\n",
    "len(news_info[\"url\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Мерджим дата фреймы\n",
    "df = news_info.merge(tweeter_data, on='url', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_date_minutes(news_date, tweet_date):\n",
    "    news_date = datetime.strptime(news_date, '%Y-%m-%d %H:%M')\n",
    "    tweet_date = datetime.strptime(tweet_date, '%Y-%m-%d %H:%M:%S')\n",
    "    return int((tweet_date-news_date).total_seconds()/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# количество минут, с момента публикации записи, данного твита\n",
    "df[\"time_since_news\"] = df.apply(lambda s: diff_date_minutes(s[\"date\"], s[\"created_at\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_week_day(date):\n",
    "    date = datetime.strptime(date, '%Y-%m-%d %H:%M')\n",
    "    return date.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# индекс дня недели\n",
    "df[\"week_day_news\"] = df.date.apply(lambda s: get_week_day(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minutes_since_midnight(date):\n",
    "    midnight = date.split(\" \")[0] + \" 00:00\"\n",
    "    date = datetime.strptime(date, '%Y-%m-%d %H:%M')\n",
    "    midnight = datetime.strptime(midnight, '%Y-%m-%d %H:%M')\n",
    "    return int((date-midnight).total_seconds()/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сколько минут времени прошло с полуночи\n",
    "df[\"minutes_since_midnight\"] = df.date.apply(lambda s: get_minutes_since_midnight(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агрегируем данные из твиттера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Время для вычисления независимых переменных\n",
    "FIRST_TIME = 10\n",
    "# Время для вычисления целевой функции\n",
    "LAST_TIME = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df[\"url\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft_df = df[df[\"time_since_news\"] <= FIRST_TIME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = ft_df.groupby(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считаем общее количество твиттов\n",
    "count_of_tweets = pd.DataFrame(grouped[\"url\"].count())\n",
    "count_of_tweets.columns = [\"first_time_tweet\"]\n",
    "count_of_tweets.reset_index(inplace=True)  \n",
    "df = pd.merge(df, count_of_tweets, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считаем общую аудиторию\n",
    "follower_sum = pd.DataFrame(grouped[\"user_followers_count\"].sum())\n",
    "follower_sum.columns = [\"follower_sum\"]\n",
    "follower_sum.reset_index(inplace=True)  \n",
    "df = pd.merge(df, follower_sum, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считаем число ретвитов\n",
    "retweeted_count_sum = pd.DataFrame(grouped[\"retweeted_count\"].sum())\n",
    "retweeted_count_sum.columns = [\"retweeted_count_sum\"]\n",
    "retweeted_count_sum.reset_index(inplace=True)  \n",
    "df = pd.merge(df, retweeted_count_sum, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считаем число звездочек\n",
    "favorite_count_sum = pd.DataFrame(grouped[\"favorite_count\"].sum())\n",
    "favorite_count_sum.columns = [\"favorite_count_sum\"]\n",
    "favorite_count_sum.reset_index(inplace=True)  \n",
    "df = pd.merge(df, favorite_count_sum, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считаем общее число списков, в которых состоят сделавшие посты\n",
    "user_listed_count = pd.DataFrame(grouped[\"user_listed_count\"].sum())\n",
    "user_listed_count.columns = [\"user_listed_count_sum\"]\n",
    "user_listed_count.reset_index(inplace=True)  \n",
    "df = pd.merge(df, user_listed_count, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_df = df[df[\"time_since_news\"] <= LAST_TIME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Выделяем значение целевой функции\n",
    "grouped = st_df.groupby(\"url\")\n",
    "count_of_tweets = pd.DataFrame(grouped[\"url\"].count())\n",
    "count_of_tweets.columns = [\"last_time_tweet\"]\n",
    "count_of_tweets.reset_index(inplace=True)  \n",
    "df = pd.merge(df, count_of_tweets, on='url', left_index=True, right_index=False, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREPARED_CSV = \"prepared_to_analys.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняемся\n",
    "df.to_csv(PREPARED_CSV, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df[\"url\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# После первого запуска, через несколько дней, запускать только часть, которая находится под этим заголовком!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOURS = 8 # Количество часов от текущего момента. Текущее время - Hours часов - время последней собранной новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качаем новости с  2015-12-04 22:20  и  2015-12-04 21:23  по  2015-12-05 11:01\n",
      "https://roem.ru/rss/roem-all-news.xml\n",
      "http://lifenews.ru/xml/feed.xml\n",
      "http://www.forbes.ru/newrss.xml\n",
      "http://www.vesti.ru/vesti.rss\n",
      "http://lenta.ru/rss\n",
      "http://ria.ru/export/rss2/index.xml\n",
      "Собрано  352  новостей с RSS\n",
      "Собрано информация о 10  новостях\n",
      "Собрано информация о 20  новостях\n",
      "Собрано информация о 30  новостях\n",
      "Собрано информация о 40  новостях\n",
      "Собрано информация о 50  новостях\n",
      "Собрано информация о 60  новостях\n",
      "Собрано информация о 70  новостях\n",
      "Собрано информация о 80  новостях\n",
      "Собрано информация о 90  новостях\n",
      "Собрано информация о 100  новостях\n",
      "Собрано информация о 110  новостях\n",
      "Собрано информация о 120  новостях\n",
      "Собрано информация о 130  новостях\n",
      "Собрано информация о 140  новостях\n",
      "Собрано информация о 150  новостях\n",
      "Собрано информация о 160  новостях\n",
      "Собрано информация о 170  новостях\n",
      "Собрано информация о 180  новостях\n",
      "[{u'message': u'Rate limit exceeded', u'code': 88}]\n",
      "Спим 723 сек.\n",
      "Собрано информация о 190  новостях\n",
      "Собрано информация о 200  новостях\n",
      "Собрано информация о 210  новостях\n",
      "Собрано информация о 220  новостях\n",
      "Собрано информация о 230  новостях\n",
      "Собрано информация о 240  новостях\n",
      "Собрано информация о 250  новостях\n",
      "Собрано информация о 260  новостях\n",
      "Собрано информация о 270  новостях\n",
      "Собрано информация о 280  новостях\n",
      "Собрано информация о 290  новостях\n",
      "Собрано информация о 300  новостях\n",
      "Собрано информация о 310  новостях\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-56ec49678151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mtw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_tweets_by_term\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Объединяем новости с предыдущими\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9e357b1e25b0>\u001b[0m in \u001b[0;36mload_tweets_by_term\u001b[1;34m(self, news_list, days_after_news)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0muntil_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdays_after_news\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadTweetWithLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muntil_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9e357b1e25b0>\u001b[0m in \u001b[0;36mloadTweetWithLink\u001b[1;34m(self, link, date)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mset_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mtweet_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muntil\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/twitter/api.pyc\u001b[0m in \u001b[0;36mGetSearch\u001b[1;34m(self, term, geocode, since_id, max_id, until, count, lang, locale, result_type, include_entities)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;31m# Make and send requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[0murl\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'%s/search/tweets.json'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mjson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RequestUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ParseAndCheckTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/twitter/api.pyc\u001b[0m in \u001b[0;36m_RequestUrl\u001b[1;34m(self, url, verb, data)\u001b[0m\n\u001b[0;32m   3600\u001b[0m           \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3601\u001b[0m           \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__auth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3602\u001b[1;33m           \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3603\u001b[0m         )\n\u001b[0;32m   3604\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;31m# By explicitly closing the session, we avoid leaving sockets open which\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# can trigger a ResourceWarning in some cases, and look like a memory leak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    466\u001b[0m         }\n\u001b[0;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    368\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m                 )\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)\u001b[0m\n\u001b[0;32m    557\u001b[0m             httplib_response = self._make_request(conn, method, url,\n\u001b[0;32m    558\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                                                   body=body, headers=headers)\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sock'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connection.pyc\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m                                     \u001b[0mca_cert_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                                     \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                                     ssl_version=resolved_ssl_version)\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.pyc\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_cert_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcertfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: OpenSSL with enabled SNI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[0;32m    350\u001b[0m                          \u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                          _context=self)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_npn_protocols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpn_protocols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context)\u001b[0m\n\u001b[0;32m    577\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/popka/anaconda/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    806\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_hours_ago(hours=3):\n",
    "    d = datetime.now() - timedelta(hours=hours)\n",
    "    d = str(d).split(\":\")\n",
    "    d = d[0]+\":\"+d[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_new_news(news_df, hours=3):\n",
    "    # Вычисляем самую молодую новость\n",
    "    tj_last_date = (news_df[news_df[\"type\"]==\"TJ_P\"][\"date\"]).max()\n",
    "    #tj_c_last_date = (news_df[news_df[\"type\"]==\"TJ_C\"][\"date\"]).max()\n",
    "    \n",
    "    #tj_last_date = max(tj_p_last_date, tj_c_last_date)\n",
    "    vc_last_date = (news_df[news_df[\"type\"]==\"VC\"][\"date\"]).max()\n",
    "    first_date = get_hours_ago(hours)\n",
    "    \n",
    "    \n",
    "    print \"Качаем новости с \", tj_last_date,\" и \", vc_last_date, \" по \", first_date\n",
    "    \n",
    "    tj_loader = TJLoader()\n",
    "    tj_pages = tj_loader.get_tj_news_info(min_index=0, count=4, first_date=first_date, last_date=tj_last_date)    \n",
    "    \n",
    "    vc_loader = VCLoader()\n",
    "    vc_pages = vc_loader.get_tj_news_info(first_date=first_date, last_date=vc_last_date)\n",
    "    \n",
    "    pages = vc_pages + tj_pages\n",
    "    pages\n",
    "    \n",
    "    return pages\n",
    "\n",
    "# Загружаем данные\n",
    "news_prev_df = pd.read_csv(OUT_NEWS_FILE, sep=\",\")\n",
    "twitter_prev_df = pd.read_csv(OUT_TWITTER_FILE, sep=\",\")\n",
    "\n",
    "# Качаем новости с VC и TJ\n",
    "pages = load_new_news(news_prev_df, HOURS)\n",
    "# Качаем новости RSS\n",
    "rss_loader = RSSLoader()\n",
    "rss_pages = rss_loader.get_news_array()\n",
    "# Объединяем\n",
    "pages += rss_pages\n",
    "\n",
    "tw = TwitterLoader()\n",
    "tweets = tw.load_tweets_by_term(pages)\n",
    "\n",
    "# Объединяем новости с предыдущими\n",
    "news = pd.DataFrame(pages)\n",
    "news = (news.append(news_prev_df)).reset_index(drop=True)\n",
    "\n",
    "# Объединяем твиты с предыдущими\n",
    "tweets_df = pd.DataFrame(tweets)\n",
    "tweets_df = (tweets_df.append(twitter_prev_df)).reset_index(drop=True)\n",
    "\n",
    "# Сохраняем\n",
    "tweets_df.to_csv(OUT_TWITTER_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "news.to_csv(OUT_NEWS_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# УДАЛЯЕМ ПОВТОРЫ =((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Сначала для твиттера\n",
    "tw_data = pd.read_csv(OUT_TWITTER_FILE, sep=\",\")\n",
    "last_size = len(tw_data)\n",
    "dupl = tw_data[\"tw_id\"].duplicated()\n",
    "dupl = np.invert((dupl.as_matrix()))\n",
    "tw_data = tw_data[dupl]\n",
    "print \"Удалено \", last_size-len(tw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем!\n",
    "tw_data.to_csv(OUT_TWITTER_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Теперь для новостей\n",
    "news_df = pd.read_csv(OUT_NEWS_FILE, sep=\",\")\n",
    "last_size = len(news_df)\n",
    "dupl = news_df[\"url\"].duplicated()\n",
    "dupl = np.invert((dupl.as_matrix()))\n",
    "news_df = news_df[dupl]\n",
    "print \"Удалено \", last_size-len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем\n",
    "news_df.to_csv(OUT_NEWS_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Здесь уже нет ничего хорошего. Уходи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = news.append(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# читаем данные\n",
    "news_df = pd.read_csv(OUT_NEWS_FILE, sep=\",\")\n",
    "twitter_df = pd.read_csv(OUT_TWITTER_FILE, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# находим самую \"молодую новость\"\n",
    "news_df = news_df.sort_values(by=[\"date\"], ascending=False)\n",
    "tj_last_date = (news_df[news_df[\"type\"]==\"TJ_P\"][\"date\"]).max()\n",
    "vc_last_date = (news_df[news_df[\"type\"]==\"VC\"][\"date\"]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tj_news_info(self, min_index=1, count=30, first_date=\"2015-11-25 12:58\", last_date=\"2015-11-29 12:58\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# скачивае информацию о новостях\n",
    "loader = TJLoader()\n",
    "pages = loader.get_tj_news_info(min_index=0, count=30, first_date=\"2015-11-25 12:58\", last_date=\"2015-11-29 12:58\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем в файлик\n",
    "news_df = news_df.append(pd.DataFrame(pages))\n",
    "news_df.to_csv(OUT_NEWS_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Скачиваем данные из твиттера\n",
    "tw = TwitterLoader()\n",
    "tweeter_data = tw.load_tweets_by_term(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tweeter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраняем в файлик\n",
    "twitter_df = twitter_df.append(pd.DataFrame(tweeter_data))\n",
    "twitter_df.to_csv(OUT_TWITTER_FILE, sep=\",\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datetime.now() - timedelta(hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = str(d).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = requests.get(\"https://api.vc.ru/1/paper\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_req = json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = html.parse(urlopen(\"https://vc.ru/p/interview-it\"))\n",
    "root = page.getroot()\n",
    "print root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = html.parse(urlopen(\"https://vc.ru/n/microsoft-store-russia\"))\n",
    "root = page.getroot()\n",
    "\n",
    "# заголовок\n",
    "title = root.find_class(\"b-article__head\")\n",
    "title = title[0].find(\"h1\").text\n",
    "print title\n",
    "\n",
    "# парсим количество просмотров\n",
    "view = root.get_element_by_id(\"hitsCount\").text\n",
    "view = view.replace(\" \", \"\")\n",
    "view = int(view)\n",
    "print view\n",
    "\n",
    "# Количество комментариев\n",
    "comments = root.find_class(\"ccount\")[0].text\n",
    "comment = int(comments.replace(\" \", \"\"))\n",
    "print comment\n",
    "\n",
    "tags = root.find_class(\"b-tags__tag\")\n",
    "tag_list = []\n",
    "for tag in tags:\n",
    "    tag_list.append(tag.text)\n",
    "    \n",
    "print tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vc = VCLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc.get_link_info(\"https://vc.ru/n/microsoft-store-russia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc_dict = vc.get_tj_news_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "1) Что делать с соурсе? Эти новости никто не репостит с TJournal, а от других источников их можно найти\n",
    "\n",
    "2) Урлы для других запросов на VC. Не хотелось бы их брудфорсить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
